# Training Configuration for SmolLM2-135M

# Model Configuration
model:
  name: "SmolLM2-135M"
  hidden_size: 576
  intermediate_size: 1536
  num_hidden_layers: 30
  num_attention_heads: 9
  num_key_value_heads: 3
  max_position_embeddings: 8192
  rope_theta: 100000.0
  rms_norm_eps: 1.0e-05
  initializer_range: 0.041666666666666664
  tie_word_embeddings: true
  hidden_act: "silu"

# Training Configuration
training:
  # Optimizer settings
  learning_rate: 3.0e-4
  weight_decay: 0.1
  adam_beta1: 0.9
  adam_beta2: 0.95
  adam_epsilon: 1.0e-8
  
  # Training steps
  max_steps: 5000
  warmup_steps: 500
  
  # Batch settings
  batch_size: 16
  gradient_accumulation_steps: 1
  
  # Sequence length
  seq_length: 256
  
  # Regularization
  gradient_clip_val: 1.0
  dropout: 0.0
  
  # Precision
  precision: "bf16-mixed"  # or "16-mixed" or "32"
  
  # Checkpointing
  save_every_n_steps: 500
  keep_top_k_checkpoints: 3
  
  # Validation
  val_check_interval: 0.25  # 4 times per epoch
  
  # Logging
  log_every_n_steps: 10
  generate_every_n_steps: 500

# Data Configuration
data:
  train_file: "input.txt"
  train_split: 0.9
  val_split: 0.1
  tokenization: "character"  # character-level for Shakespeare
  
# Generation Configuration (for monitoring)
generation:
  max_length: 200
  temperature: 0.8
  top_k: 40
  top_p: 0.95
  do_sample: true

# Hardware Configuration
hardware:
  accelerator: "gpu"  # or "cpu"
  devices: 1
  num_workers: 0  # DataLoader workers (0 for Colab)
  pin_memory: true

# Reproducibility
seed: 42

# Optimization Features Enabled
optimizations:
  weight_sharing: true  # tie_word_embeddings
  residual_scaling: true  # 1/sqrt(2*num_layers)
  grouped_query_attention: true  # 9 heads share 3 KV heads
  mixed_precision: true  # bfloat16
  gradient_clipping: true  # clip at 1.0
  flash_attention: false  # Not implemented yet
